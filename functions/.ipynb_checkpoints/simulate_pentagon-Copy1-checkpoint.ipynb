{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09ed4d83-7343-411b-aeca-2293cebad711",
   "metadata": {},
   "source": [
    "# Simulate_pentagon.ipynb\n",
    "\n",
    "## Purpose\n",
    "This notebook encompasses the generation of synthetic drawings of intersecting pentagons, as well as the prediction of global cognitive scores given the synthetic images.\n",
    "\n",
    "## Method\n",
    "The simulator employs eight parameters, including (1) angle distortion, (2) line waviness, (3) line width, (4) the number of vertices, (5) alignment of two pentagons, (6) the distance between pentagons, (7) pentagon size, and (8) size equality. For each value of the primary parameter tested, we generated 20 images with slight variations by randomly varying the remaining parameters within a small range around regular pentagons. The generated images were then supplied to ten deep learning models, each of which had been trained using a distinct subset of data.\n",
    "\n",
    "\n",
    "## Result\n",
    "All results can be found within the output directory. The list of parameters utilized for generating each image is stored as param.txt, and the predicted cognitive values for the simulated images are stored as {model_name}.txt.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e75a06-e62b-4e59-b019-f1678157c15a",
   "metadata": {},
   "source": [
    "## Local library import\n",
    "We import all the required local libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb9d75bd-044c-4a89-8f14-8a814f05cbc1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import wandb\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import copy\n",
    "import time\n",
    "import scipy.stats\n",
    "import pandas as pd\n",
    "import os \n",
    "import numpy as np\n",
    "import munch\n",
    "import matplotlib.pyplot\n",
    "import matplotlib\n",
    "import json\n",
    "import imgaug as ia\n",
    "import imgaug as ia\n",
    "import imgaug\n",
    "import h5py\n",
    "import glob\n",
    "import gc\n",
    "import argparse\n",
    "from torchvision import models, transforms\n",
    "from torch.utils import data\n",
    "from simulate_pentagon import simulate_pentagon\n",
    "from PIL import Image\n",
    "from imgaug import augmenters as iaa\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba186aba-08a0-46e2-acd0-42a7b0ba31ba",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Define Functions\n",
    "In this section, we outline the local functions utilized for the analysis.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85c042d0-c634-4550-8414-6c44cec06f56",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def initialize_model(model_name, feature_extract, use_pretrained=True):\n",
    "    # Initialize these variables which will be set in this if statement. Each of these\n",
    "    #   variables is model specific.\n",
    "    model_ft = None\n",
    "    input_size = 0\n",
    "    num_classes = 1\n",
    "\n",
    "    if ('resnet' in model_name) & (not ('wide' in model_name)):\n",
    "        if model_name == \"resnet18\":\n",
    "            \"\"\" Resnet18\n",
    "            \"\"\"\n",
    "            model_ft = models.resnet18(pretrained=use_pretrained)\n",
    "        elif model_name == \"resnet34\":\n",
    "            \"\"\" Resnet34\n",
    "            \"\"\"\n",
    "            model_ft = models.resnet34(pretrained=use_pretrained)\n",
    "        elif model_name == \"resnet50\":\n",
    "            \"\"\" Resnet50\n",
    "            \"\"\"\n",
    "            model_ft = models.resnet50(pretrained=use_pretrained)\n",
    "        elif model_name == \"resnet101\":\n",
    "            \"\"\" Resnet101\n",
    "            \"\"\"\n",
    "            model_ft = models.resnet101(pretrained=use_pretrained)\n",
    "        elif model_name == \"resnet152\":\n",
    "            \"\"\" Resnet152\n",
    "            \"\"\"\n",
    "            model_ft = models.resnet152(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.fc.in_features\n",
    "        model_ft.fc = nn.Linear(num_ftrs, num_classes)\n",
    "        input_size = 224\n",
    "    elif model_name == \"googlenet\":\n",
    "        \"\"\" googlenet\n",
    "        \"\"\"\n",
    "        model_ft = models.googlenet(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.fc.in_features\n",
    "        model_ft.fc = nn.Linear(num_ftrs, num_classes)\n",
    "        input_size = 224\n",
    "    elif model_name == \"alexnet\":\n",
    "        \"\"\" Alexnet\n",
    "        \"\"\"\n",
    "        model_ft = models.alexnet(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier[6].in_features\n",
    "        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
    "        input_size = 224\n",
    "    elif 'wide_resnet' in model_name:\n",
    "        if model_name == \"wide_resnet50_2\":\n",
    "            \"\"\" wide_resnet50_2\n",
    "            \"\"\"\n",
    "            model_ft = models.wide_resnet50_2(pretrained=use_pretrained)\n",
    "        elif model_name == \"wide_resnet101_2\":\n",
    "            \"\"\" wide_resnet101_2\n",
    "            \"\"\"\n",
    "            model_ft = models.wide_resnet101_2(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.fc.in_features\n",
    "        model_ft.fc = nn.Linear(num_ftrs, num_classes)\n",
    "        input_size = 224\n",
    "    elif 'resnext' in model_name:\n",
    "        if model_name == \"resnext50_32x4d\":\n",
    "            \"\"\" resnext50_32x4d\n",
    "            \"\"\"\n",
    "            model_ft = models.resnext50_32x4d(pretrained=use_pretrained)\n",
    "        elif model_name == \"resnext101_32x8d\":\n",
    "            \"\"\" resnext101_32x8d\n",
    "            \"\"\"\n",
    "            model_ft = models.resnext101_32x8d(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.fc.in_features\n",
    "        model_ft.fc = nn.Linear(num_ftrs, num_classes)\n",
    "        input_size = 224\n",
    "    elif 'mnasnet' in model_name:\n",
    "        if model_name == \"mnasnet0_5\":\n",
    "            \"\"\" mnasnet0_5\n",
    "            \"\"\"\n",
    "            model_ft = models.mnasnet0_5(pretrained=use_pretrained)\n",
    "        elif model_name == \"mnasnet0_75\":\n",
    "            \"\"\" mnasnet0_75\n",
    "            \"\"\"\n",
    "            model_ft = models.mnasnet0_75(pretrained=use_pretrained)\n",
    "        elif model_name == \"mnasnet1_0\":\n",
    "            \"\"\" mnasnet1_0\n",
    "            \"\"\"\n",
    "            model_ft = models.mnasnet1_0(pretrained=use_pretrained)\n",
    "        elif model_name == \"mnasnet1_3\":\n",
    "            \"\"\" mnasnet1_3\n",
    "            \"\"\"\n",
    "            model_ft = models.mnasnet1_3(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier[1].in_features\n",
    "        model_ft.classifier[1] = nn.Linear(num_ftrs, num_classes)\n",
    "        input_size = 224\n",
    "    elif 'mobilenet_v3' in model_name:\n",
    "        if model_name == \"mobilenet_v3_large\":\n",
    "            \"\"\" mobilenet_v3_large\n",
    "            \"\"\"\n",
    "            model_ft = models.mobilenet_v3_large(pretrained=use_pretrained)\n",
    "        elif model_name == \"mobilenet_v3_small\":\n",
    "            \"\"\" mobilenet_v3_small\n",
    "            \"\"\"\n",
    "            model_ft = models.mobilenet_v3_small(pretrained=use_pretrained)\n",
    "            \n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier[3].in_features\n",
    "        model_ft.classifier[3] = nn.Linear(num_ftrs, num_classes)\n",
    "        input_size = 224\n",
    "    elif 'mobilenet_v2' in model_name:\n",
    "        model_ft = models.mobilenet_v2(pretrained=use_pretrained)\n",
    "        num_ftrs = model_ft.classifier[1].in_features\n",
    "        model_ft.classifier[1] = nn.Linear(num_ftrs, num_classes)\n",
    "        input_size = 224\n",
    "    elif 'vgg' in model_name:\n",
    "        if model_name == \"vgg11\":\n",
    "            \"\"\" VGG11\n",
    "            \"\"\"\n",
    "            model_ft = models.vgg11(pretrained=use_pretrained)\n",
    "        elif model_name == \"vgg11_bn\":\n",
    "            \"\"\" VGG11_bn\n",
    "            \"\"\"\n",
    "            model_ft = models.vgg11_bn(pretrained=use_pretrained)\n",
    "        elif model_name == \"vgg13\":\n",
    "            \"\"\" VGG13\n",
    "            \"\"\"\n",
    "            model_ft = models.vgg13(pretrained=use_pretrained)\n",
    "        elif model_name == \"vgg13_bn\":\n",
    "            \"\"\" VGG13_bn\n",
    "            \"\"\"\n",
    "            model_ft = models.vgg13_bn(pretrained=use_pretrained)\n",
    "        elif model_name == \"vgg16\":\n",
    "            \"\"\" VGG16\n",
    "            \"\"\"\n",
    "            model_ft = models.vgg16(pretrained=use_pretrained)\n",
    "        elif model_name == \"vgg16_bn\":\n",
    "            \"\"\" VGG16_bn\n",
    "            \"\"\"\n",
    "            model_ft = models.vgg16_bn(pretrained=use_pretrained)\n",
    "        elif model_name == \"vgg19\":\n",
    "            \"\"\" VGG19\n",
    "            \"\"\"\n",
    "            model_ft = models.vgg19(pretrained=use_pretrained)\n",
    "        elif model_name == \"vgg19_bn\":\n",
    "            \"\"\" VGG19_bn\n",
    "            \"\"\"\n",
    "            model_ft = models.vgg19_bn(pretrained=use_pretrained)\n",
    "            \n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier[6].in_features\n",
    "        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
    "        input_size = 224\n",
    "    elif 'efficientnet' in model_name:\n",
    "        if model_name == \"efficientnet_b0\":\n",
    "            \"\"\" efficientnet_b0\n",
    "            \"\"\"\n",
    "            model_ft = models.efficientnet_b0(pretrained=use_pretrained)\n",
    "        elif model_name == \"efficientnet_b1\":\n",
    "            \"\"\" efficientnet_b1\n",
    "            \"\"\"\n",
    "            model_ft = models.efficientnet_b1(pretrained=use_pretrained)\n",
    "        elif model_name == \"efficientnet_b2\":\n",
    "            \"\"\" efficientnet_b2\n",
    "            \"\"\"\n",
    "            model_ft = models.efficientnet_b2(pretrained=use_pretrained)\n",
    "        elif model_name == \"efficientnet_b3\":\n",
    "            \"\"\" efficientnet_b3\n",
    "            \"\"\"\n",
    "            model_ft = models.efficientnet_b3(pretrained=use_pretrained)\n",
    "        elif model_name == \"efficientnet_b4\":\n",
    "            \"\"\" efficientnet_b4\n",
    "            \"\"\"\n",
    "            model_ft = models.efficientnet_b4(pretrained=use_pretrained)\n",
    "        elif model_name == \"efficientnet_b5\":\n",
    "            \"\"\" efficientnet_b5\n",
    "            \"\"\"\n",
    "            model_ft = models.efficientnet_b5(pretrained=use_pretrained)\n",
    "        elif model_name == \"efficientnet_b6\":\n",
    "            \"\"\" efficientnet_b6\n",
    "            \"\"\"\n",
    "            model_ft = models.efficientnet_b6(pretrained=use_pretrained)\n",
    "        elif model_name == \"efficientnet_b7\":\n",
    "            \"\"\" efficientnet_b7\n",
    "            \"\"\"\n",
    "            model_ft = models.efficientnet_b7(pretrained=use_pretrained)\n",
    "            \n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier[1].in_features\n",
    "        model_ft.classifier[1] = nn.Linear(num_ftrs,num_classes)\n",
    "        input_size = 224\n",
    "    elif 'regnet' in model_name:\n",
    "        if model_name == \"regnet_y_400mf\":\n",
    "            \"\"\" regnet_y_400mf\n",
    "            \"\"\"\n",
    "            model_ft = models.regnet_y_400mf(pretrained=use_pretrained)\n",
    "        elif model_name == \"regnet_y_800mf\":\n",
    "            \"\"\" regnet_y_800mf\n",
    "            \"\"\"\n",
    "            model_ft = models.regnet_y_800mf(pretrained=use_pretrained)\n",
    "        elif model_name == \"regnet_y_1_6gf\":\n",
    "            \"\"\" regnet_y_1_6gf\n",
    "            \"\"\"\n",
    "            model_ft = models.regnet_y_1_6gf(pretrained=use_pretrained)\n",
    "        elif model_name == \"regnet_y_3_2gf\":\n",
    "            \"\"\" regnet_y_3_2gf\n",
    "            \"\"\"\n",
    "            model_ft = models.regnet_y_3_2gf(pretrained=use_pretrained)\n",
    "        elif model_name == \"regnet_y_8gf\":\n",
    "            \"\"\" regnet_y_8gf\n",
    "            \"\"\"\n",
    "            model_ft = models.regnet_y_8gf(pretrained=use_pretrained)\n",
    "        elif model_name == \"regnet_y_16gf\":\n",
    "            \"\"\" regnet_y_16gf\n",
    "            \"\"\"\n",
    "            model_ft = models.regnet_y_16gf(pretrained=use_pretrained)\n",
    "        elif model_name == \"regnet_y_32gf\":\n",
    "            \"\"\" regnet_y_32gf\n",
    "            \"\"\"\n",
    "            model_ft = models.regnet_y_32gf(pretrained=use_pretrained)\n",
    "        elif model_name == \"regnet_x_400mf\":\n",
    "            \"\"\" regnet_x_400mf\n",
    "            \"\"\"\n",
    "            model_ft = models.regnet_x_400mf(pretrained=use_pretrained)\n",
    "        elif model_name == \"regnet_x_800mf\":\n",
    "            \"\"\" regnet_x_800mf\n",
    "            \"\"\"\n",
    "            model_ft = models.regnet_x_800mf(pretrained=use_pretrained)\n",
    "        elif model_name == \"regnet_x_1_6gf\":\n",
    "            \"\"\" regnet_x_1_6gf\n",
    "            \"\"\"\n",
    "            model_ft = models.regnet_x_1_6gf(pretrained=use_pretrained)\n",
    "        elif model_name == \"regnet_x_3_2gf\":\n",
    "            \"\"\" regnet_x_3_2gf\n",
    "            \"\"\"\n",
    "            model_ft = models.regnet_x_3_2gf(pretrained=use_pretrained)\n",
    "        elif model_name == \"regnet_x_8gf\":\n",
    "            \"\"\" regnet_x_8gf\n",
    "            \"\"\"\n",
    "            model_ft = models.regnet_x_8gf(pretrained=use_pretrained)\n",
    "        elif model_name == \"regnet_x_16gf\":\n",
    "            \"\"\" regnet_x_16gf\n",
    "            \"\"\"\n",
    "            model_ft = models.regnet_x_16gf(pretrained=use_pretrained)\n",
    "        elif model_name == \"regnet_x_32gf\":\n",
    "            \"\"\" regnet_x_32gf\n",
    "            \"\"\"\n",
    "            model_ft = models.regnet_x_32gf(pretrained=use_pretrained)\n",
    "            \n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.fc.in_features\n",
    "        model_ft.fc = nn.Linear(num_ftrs,num_classes)\n",
    "        input_size = 224\n",
    "    elif model_name == \"squeezenet1_1\":\n",
    "        \"\"\" Squeezenet\n",
    "        \"\"\"\n",
    "        model_ft = models.squeezenet1_1(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        model_ft.classifier[1] = nn.Conv2d(512, num_classes, kernel_size=(1,1), stride=(1,1))\n",
    "        model_ft.num_classes = num_classes\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"densenet121\":\n",
    "        \"\"\" Densenet\n",
    "        \"\"\"\n",
    "        model_ft = models.densenet121(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier.in_features\n",
    "        model_ft.classifier = nn.Linear(num_ftrs, num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"inception\":\n",
    "        \"\"\" Inception v3\n",
    "        Be careful, expects (299,299) sized images and has auxiliary output\n",
    "        \"\"\"\n",
    "        model_ft = models.inception_v3(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        # Handle the auxilary net\n",
    "        num_ftrs = model_ft.AuxLogits.fc.in_features\n",
    "        model_ft.AuxLogits.fc = nn.Linear(num_ftrs, num_classes)\n",
    "        # Handle the primary net\n",
    "        num_ftrs = model_ft.fc.in_features\n",
    "        model_ft.fc = nn.Linear(num_ftrs,num_classes)\n",
    "        input_size = 299\n",
    "\n",
    "    else:\n",
    "        print(\"Invalid model name, exiting...\")\n",
    "        exit()\n",
    "\n",
    "    return model_ft, input_size\n",
    "def set_parameter_requires_grad(model, feature_extracting):\n",
    "    if feature_extracting:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "class ImgAugTransform:\n",
    "    def __init__(self,params):\n",
    "        \n",
    "        aug_list = []\n",
    "        if params['Fliplr']:\n",
    "            aug_list.append(iaa.Fliplr(0.5))\n",
    "        if params['Flipud']:\n",
    "            aug_list.append(iaa.Flipud(0.5))\n",
    "        if params['Rot90']:\n",
    "            aug_list.append(iaa.Rot90((0,3)))\n",
    "        if params['Affine']:\n",
    "            aug_list.append(iaa.Sometimes(params['aff_sometimes'],\n",
    "                                          iaa.Affine(translate_percent={'x':(-params['aff_translate'],\n",
    "                                                                             params['aff_translate']),\n",
    "                                                                        'y':(-params['aff_translate'],\n",
    "                                                                             params['aff_translate'])},\n",
    "                                                     rotate=(-params['aff_rotate'], params['aff_rotate']),\n",
    "                                                     cval=255)))\n",
    "        aug_list2 = []\n",
    "        if params['CBS']:\n",
    "            aug_list2.append(iaa.SomeOf((0, 3), [\n",
    "                iaa.GammaContrast((0, 2.0)),\n",
    "                iaa.GaussianBlur(sigma=(0, 3.0)),\n",
    "                iaa.Sharpen((0, 1))\n",
    "            ],random_order=True))\n",
    "        if params['AdditiveGaussianNoise']:\n",
    "            aug_list2.append(iaa.AdditiveGaussianNoise(scale=(0, 0.02*255)))\n",
    "        if params['SaltAndPepper']:\n",
    "            aug_list2.append(iaa.SaltAndPepper(0.05))\n",
    "        \n",
    "        self.aug = iaa.Sequential(aug_list, \n",
    "                                  random_order=True)\n",
    "        self.aug2 = iaa.Sequential(aug_list2, \n",
    "                                  random_order=True)\n",
    "    def __call__(self, img):\n",
    "        img = np.array(img)\n",
    "        img = self.aug.augment_image(img)\n",
    "        img = self.aug2.augment_image(img)\n",
    "        return img\n",
    "\n",
    "def define_network(config):\n",
    "    net, input_size = initialize_model(config.model_name, False, use_pretrained=True)\n",
    "    net.to(config.device)\n",
    "    return net, input_size\n",
    "\n",
    "def run_exp(net,param_list,label,param_range,config):\n",
    "    if not os.path.exists(config['output_dir']+label+\"/images/\"):\n",
    "        os.makedirs(config['output_dir']+label+\"/images/\")\n",
    "    image = [simulate_pentagon(**param_list[i]) for i in range(len(param_list))]\n",
    "    pd.DataFrame(param_list).to_csv(config['output_dir']+label+\"/param.txt\",sep=\"\\t\")\n",
    "    for i in range(len(image)):\n",
    "        image[i].save(config['output_dir']+label+\"/images/\"+str(i)+\".png\", format=\"png\")\n",
    "    ia.seed(1234)\n",
    "    image = torch.stack([transform_img(Image.fromarray(transforms_imgaug(image[i]))) for i in range(len(image))])\n",
    "\n",
    "    for j in range(1,config[\"num_models\"]+1):\n",
    "        #print(j)\n",
    "        # Load model\n",
    "        config[\"model_loc\"]=\"../models/vgg19_bn-\"+str(j)+\"_model.pth\"\n",
    "        net.load_state_dict(torch.load(config[\"model_loc\"],\n",
    "                                       map_location=config.device))\n",
    "        with torch.no_grad():\n",
    "            net.eval()\n",
    "            out = net(image.cuda())\n",
    "            out = out.cpu().detach().numpy()\n",
    "        pd.DataFrame({'n':param_range,\n",
    "                     'pred':out[:,0]}).to_csv(config['output_dir']+label+\"/\"+os.path.basename(config['model_loc']).replace(\".pth\",\".txt\"),\n",
    "               sep=\"\\t\")\n",
    "    del image\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac7c81d-12fc-4f72-8f7e-5f52d53e0e0d",
   "metadata": {},
   "source": [
    "# Parameter Specification\n",
    "In this section, we establish all crucial parameters for our notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5dafe72-299a-44cd-8eab-5917534bd9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'model_name': \"vgg19_bn\",\n",
    "    'batch_size': 32,\n",
    "    'Fliplr':True,\n",
    "    'Flipud':True,\n",
    "    'Rot90':True,\n",
    "    'Affine':True,\n",
    "    'aff_sometimes':0.9,\n",
    "    'aff_translate':0.1,\n",
    "    'aff_rotate':10,\n",
    "    'CBS':True,\n",
    "    'AdditiveGaussianNoise':False,\n",
    "    'SaltAndPepper':False,\n",
    "    'output_dir': \"../results/simulate_pentagon/\",\n",
    "    'num_models':10\n",
    "}\n",
    "config=munch.Munch(config)\n",
    "config.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4ca18f-34bd-4d6f-9bef-6edbd36c8542",
   "metadata": {},
   "source": [
    "# Model Architecture Loading\n",
    "In this step, we import a model from torchvision and adjust the final output layer accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9e6d056-22c5-4ebb-8e2b-8dd1d242596e",
   "metadata": {},
   "outputs": [],
   "source": [
    "net, input_size = define_network(config)\n",
    "config.input_size = input_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a51819-d8ba-4201-a8ed-49e0c03839d9",
   "metadata": {},
   "source": [
    "# Image Processing Pipeline Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3262601-867b-4e0c-90a3-012ae0406f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_img = transforms.Compose([transforms.Resize((config.input_size,config.input_size)),\n",
    "                                             transforms.ToTensor(),\n",
    "                                             transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                                                                  [0.229, 0.224, 0.225])])\n",
    "transforms_imgaug = ImgAugTransform(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929af822-1560-4418-939c-cdf5389c36ac",
   "metadata": {},
   "source": [
    "# Simulation Experiment Execution\n",
    "In this section, we execute the pentagon simulation and predict the global cognitive score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6896c9f4-986f-4591-8153-3cb309c0b0a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.load_state_dict(torch.load(\"/mnt/new_disk/006_DeepPentagon/Clean_code/DeepRoku/models/vgg19_bn-1_model.pth\", map_location=config.device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d55b6256-cf29-4d7a-930d-938b3f1d0e24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_vertex\n"
     ]
    }
   ],
   "source": [
    "torch.backends.cudnn.benchmark = True\n",
    "label=\"n_vertex\"\n",
    "print(label)\n",
    "np.random.seed(1234)\n",
    "param_range = np.repeat(np.arange(3, 11, 1),20)\n",
    "param_list = [{'n':i,\n",
    "  'pentagon_size' : np.random.uniform(0.8,1.2),\n",
    "  'rot' : np.random.uniform(0,5),\n",
    "  'lw' : np.random.uniform(1,2),\n",
    "  'dist' : np.random.uniform(0.8,1.2),\n",
    "  'rot_right': np.random.uniform(-10,10),\n",
    "  'size_right':np.random.uniform(0.8,1.2),\n",
    "  'rot_both':0,\n",
    "  'line_randomness':np.random.uniform(1,2)} for i in param_range]\n",
    "\n",
    "np.random.seed(1234)\n",
    "param_range = np.repeat(np.arange(3, 11, 1),1)\n",
    "param_list = [{'n':i,\n",
    "  'pentagon_size' : 1.3,\n",
    "  'rot' : 0,\n",
    "  'lw' : 2,\n",
    "  'dist' : 0.9,\n",
    "  'rot_right': 0,\n",
    "  'size_right':1,\n",
    "  'rot_both':0,\n",
    "  'line_randomness':1} for i in param_range]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "74c05fa1-0530-42a4-b369-3e8ba00de220",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /opt/conda/conda-bld/pytorch_1623448265233/work/c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    }
   ],
   "source": [
    "    image = [simulate_pentagon(**param_list[i]) for i in range(len(param_list))]\n",
    "    #pd.DataFrame(param_list).to_csv(\"../results/Interpretation_simulate/\"+label+\"/param.txt\",sep=\"\\t\")\n",
    "    #for i in range(len(image)):\n",
    "    #    image[i].save(\"../results/Interpretation_simulate/\"+label+\"/\"+str(i)+\".png\", format=\"png\")\n",
    "    ia.seed(1234)\n",
    "    #image = torch.stack([transform_img(Image.fromarray(transforms_imgaug(image[i]))) for i in range(len(image))])\n",
    "    image = torch.stack([transform_img(image[i]) for i in range(len(image))])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        net.eval()\n",
    "        out = net(image.cuda())\n",
    "        out = out.cpu().detach().numpy()\n",
    "        #image = image.cpu().detach()\n",
    "        #del image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0d9a9d87-0728-4663-8158-4245051e5c6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.40395734,  0.5906327 ,  0.6153195 ,  0.8285357 ,  0.67948496,\n",
       "        0.60409415,  0.5761483 ,  0.49812192], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[0:10,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ee6e3df2-1332-4a10-aa3b-6e30c7dbc0d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.38759255,  0.06278662,  0.627287  ,  0.87936896,  0.53538686,\n",
       "        0.32392937,  0.2190382 , -0.05131597], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[0:10,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b3d41baf-1df8-4698-b68a-92b11edac017",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.91971254, -0.3940602 , -0.4728278 , -0.70505536, -0.46476603,\n",
       "       -0.49545592, -0.56187034, -0.6766908 , -0.68769354, -0.94573694],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[0:10,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "970265df-29bd-40fb-8168-407e954b4004",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.7246869 , -0.580871  , -0.6101955 , -1.3135885 , -1.1198303 ,\n",
       "       -0.8103528 , -0.93835866, -1.0705982 , -0.68374354, -1.5658635 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[0:10,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b51786e7-2b48-481d-bd63-ff89e90008d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.7246869 , -0.580871  , -0.6101955 , -1.3135885 , -1.1198303 ,\n",
       "       -0.8103528 , -0.93835866, -1.0705982 , -0.68374354, -1.5658635 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[0:10,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fa8e99f3-ff39-4d88-8a08-da76c9d6d5dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(110087.1406)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(sum(image[0,0,:,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "06ad236d-efb5-4e58-93a6-c1fdbd127cbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(111419.6172)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(sum(image[0,0,:,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "950e6e1c-f6ae-4782-b247-ec7bfb81c093",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImgAugTransform:\n",
    "    def __init__(self,params):\n",
    "        \n",
    "        aug_list = []\n",
    "        if params['Fliplr']:\n",
    "            aug_list.append(iaa.Fliplr(0.5))\n",
    "        if params['Flipud']:\n",
    "            aug_list.append(iaa.Flipud(0.5))\n",
    "        if params['Rot90']:\n",
    "            aug_list.append(iaa.Rot90((0,3)))\n",
    "        if params['Affine']:\n",
    "            aug_list.append(iaa.Sometimes(params['aff_sometimes'],\n",
    "                                          iaa.Affine(translate_percent={'x':(-params['aff_translate'],\n",
    "                                                                             params['aff_translate']),\n",
    "                                                                        'y':(-params['aff_translate'],\n",
    "                                                                             params['aff_translate'])},\n",
    "                                                     rotate=(-params['aff_rotate'], params['aff_rotate']),\n",
    "                                                     cval=255)))\n",
    "        aug_list2 = []\n",
    "        if params['CBS']:\n",
    "            aug_list2.append(iaa.SomeOf((0, 3), [\n",
    "                iaa.GammaContrast((0, 2.0)),\n",
    "                iaa.GaussianBlur(sigma=(0, 3.0)),\n",
    "                iaa.Sharpen((0, 1))\n",
    "            ],random_order=True))\n",
    "        if params['AdditiveGaussianNoise']:\n",
    "            aug_list2.append(iaa.AdditiveGaussianNoise(scale=(0, 0.02*255)))\n",
    "        if params['SaltAndPepper']:\n",
    "            aug_list2.append(iaa.SaltAndPepper(0.05))\n",
    "        \n",
    "        self.aug = iaa.Sequential(aug_list, \n",
    "                                  random_order=True)\n",
    "        self.aug2 = iaa.Sequential(aug_list2, \n",
    "                                  random_order=True)\n",
    "    def __call__(self, img):\n",
    "        img = np.array(img)\n",
    "        img = self.aug.augment_image(img)\n",
    "        img = self.aug2.augment_image(img)\n",
    "        return img\n",
    "\n",
    "\n",
    "class Dataset(data.Dataset):\n",
    "    'Characterizes a dataset for PyTorch'\n",
    "    def __init__(self,params, aug):\n",
    "        'Initialization'\n",
    "        pheno=pd.read_csv(params[\"pheno_file\"],sep=\"\\t\")\n",
    "        pheno.iloc[:,0] = [x.replace(\"jpg\",\"png\") for x in pheno.values[:,0]]\n",
    "        self.image_file_dir = params[\"image_file_dir\"]\n",
    "        self.pheno = torch.from_numpy(np.array(pheno.values[:,1]).astype(np.float64))\n",
    "        self.file_name = np.array(pheno.values[:,0])\n",
    "        self.transform_img = transforms.Compose([transforms.Resize((params[\"input_size\"],params[\"input_size\"])),\n",
    "                                                 transforms.ToTensor(),\n",
    "                                                 transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                                                                      [0.229, 0.224, 0.225])])\n",
    "        self.aug=aug\n",
    "        if self.aug:\n",
    "            self.transforms_imgaug = ImgAugTransform(params)\n",
    "    def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        return self.pheno.shape[0]\n",
    "    def __getitem__(self, index):\n",
    "        'Generates one sample of data'\n",
    "        # Load data and get label\n",
    "        X = Image.open(self.image_file_dir+self.file_name[index], 'r')\n",
    "        if self.aug:\n",
    "            X = self.transforms_imgaug(X)\n",
    "            X = Image.fromarray(X)\n",
    "        X = self.transform_img(X)\n",
    "        y = self.pheno[index]\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1235bdf7-d186-4db0-a30a-994690d3654d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /opt/conda/conda-bld/pytorch_1623448265233/work/c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    }
   ],
   "source": [
    "    torch.backends.cudnn.benchmark = True\n",
    "    # testing\n",
    "    # Load best model\n",
    "    params_dl = {'batch_size': config.batch_size,\n",
    "                 'shuffle': False,\n",
    "                 'num_workers': os.cpu_count(),\n",
    "                 'pin_memory':True}\n",
    "    config[\"pheno_file\"]=\"/mnt/new_disk/006_DeepPentagon/Data/TestPentagonImagesPhenotype_R2_dropiregs/cogn_global/1/test.txt\"\n",
    "    config[\"image_file_dir\"]=\"/mnt/new_disk/006_DeepPentagon/Analysis/01_Image_prep/TestPentagonImages/\"\n",
    "    test_generator = data.DataLoader(Dataset(config,aug=False), **params_dl)\n",
    "    y_pred=[]\n",
    "    with torch.no_grad():\n",
    "        for i,(X, y) in enumerate(test_generator):\n",
    "            X = X.to(config.device, non_blocking=True)\n",
    "            # Model computation\n",
    "            net.eval()\n",
    "            output = net(X)\n",
    "            y_pred.append(np.array(output.cpu().detach()[:,0]))\n",
    "    del X, y, test_generator, output\n",
    "    y_pred=np.concatenate(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "58674005-b5bc-4fa0-8e87-ed8b68c82deb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.9719458 , -0.04865612,  0.34724107, ...,  0.01954526,\n",
       "        0.20850907,  0.11958006], dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "af3e60f7-8713-408c-80f3-51d0d9cb050b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.9719458 , -0.04865611,  0.34724113, ...,  0.01954481,\n",
       "        0.20850952,  0.11957934], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
